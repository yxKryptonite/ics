#######################################################################
# Test for copying block of size 63;
#######################################################################
	.pos 0
main:	irmovq Stack, %rsp  	# Set up stack pointer

	# Set up arguments for copy function and then invoke it
	irmovq $63, %rdx		# src and dst have 63 elements
	irmovq dest, %rsi	# dst array
	irmovq src, %rdi	# src array
	call ncopy		 
	halt			# should halt with num nonzeros in %rax
StartFun:
#/* $begin ncopy-ys */
##################################################################
# ncopy.ys - Copy a src block of len words to dst.
# Return the number of positive words (>0) contained in src.
#
# Include your name and ID here.
# Yuxuan Kuang 2100013089
# Describe how and why you modified the baseline code.
# First, I add the instruction `iaddq` in the HCL file to replace `addq`.
# Then I unroll the loop into 10 iterations.
# For the remaining number: construct a trinary search tree to set jump.
# The Trinary Tree is like:
# 		  ROOT
# 	   /   |    \
#     l    3     r
#    / \       / | \
#   1   2    rl  6  rr
#           / \     /|\
#          4   5   7 8 9
##################################################################
# Do not modify this portion
# Function prologue.
# %rdi = src, %rsi = dst, %rdx = len
# tmp: r8, ...
ncopy:

##################################################################
	xorq %rax,%rax
	iaddq $-10, %rdx
	jl root
	
	iter1:
		mrmovq (%rdi), %r8
		mrmovq 8(%rdi), %r9
		rmmovq %r8, (%rsi)
		andq %r8, %r8
		jle iter2
		iaddq $1, %rax

	iter2:
		mrmovq 16(%rdi), %r8
		rmmovq %r9, 8(%rsi)
		andq %r9, %r9
		jle iter3
		iaddq $1, %rax

	iter3:
		mrmovq 24(%rdi), %r9
		rmmovq %r8, 16(%rsi)
		andq %r8, %r8
		jle iter4
		iaddq $1, %rax

	iter4:
		mrmovq 32(%rdi), %r8
		rmmovq %r9, 24(%rsi)
		andq %r9, %r9
		jle iter5
		iaddq $1, %rax
	
	iter5:
		mrmovq 40(%rdi), %r9
		rmmovq %r8, 32(%rsi)
		andq %r8, %r8
		jle iter6
		iaddq $1, %rax

	iter6:
		mrmovq 48(%rdi), %r8
		rmmovq %r9, 40(%rsi)
		andq %r9, %r9
		jle iter7
		iaddq $1, %rax

	iter7:
		mrmovq 56(%rdi), %r9
		rmmovq %r8, 48(%rsi)
		andq %r8, %r8
		jle iter8
		iaddq $1, %rax

	iter8:
		mrmovq 64(%rdi), %r8
		rmmovq %r9, 56(%rsi)
		andq %r9, %r9
		jle iter9
		iaddq $1, %rax

	iter9:
		mrmovq 72(%rdi), %r9
		rmmovq %r8, 64(%rsi)
		andq %r8, %r8
		jle iter10
		iaddq $1, %rax

	iter10:
		# mrmovq 80(%rdi), %r8
		rmmovq %r9, 72(%rsi)
		andq %r9, %r9
		jle loop
		iaddq $1, %rax

	loop:
		iaddq $80, %rdi
		iaddq $80, %rsi
		iaddq $-10, %rdx
		jge iter1

	root:
		iaddq $7, %rdx
		jl left_child
		jg right_child
		je middle_child_r3

	left_child:
		iaddq $2, %rdx
		je r1
		iaddq $-1, %rdx
		je r2
		ret

	right_child:
		iaddq $-3, %rdx
		jg rr
		je r6
		iaddq $1, %rdx
		je r5
		jmp r4
	
	rr:
		iaddq $-2, %rdx
		je r8
		jl r7

	r9:
		mrmovq 64(%rdi), %r8
		rmmovq %r8, 64(%rsi)
		andq %r8, %r8

	r8:
		mrmovq 56(%rdi), %r8
		jle r8_
		iaddq $1, %rax

	r8_:
		rmmovq %r8, 56(%rsi)
		andq %r8, %r8

	r7:
		mrmovq 48(%rdi), %r8
		jle r7_
		iaddq $1, %rax

	r7_:
		rmmovq %r8, 48(%rsi)
		andq %r8, %r8

	r6:
		mrmovq 40(%rdi), %r8
		jle r6_
		iaddq $1, %rax

	r6_:
		rmmovq %r8, 40(%rsi)
		andq %r8, %r8

	r5:
		mrmovq 32(%rdi), %r8
		jle r5_
		iaddq $1, %rax

	r5_:
		rmmovq %r8, 32(%rsi)
		andq %r8, %r8

	r4:
		mrmovq 24(%rdi), %r8
		jle r4_
		iaddq $1, %rax

	r4_:
		rmmovq %r8, 24(%rsi)
		andq %r8, %r8

	middle_child_r3:
		mrmovq 16(%rdi), %r8
		jle r3_
		iaddq $1, %rax

	r3_:
		rmmovq %r8, 16(%rsi)
		andq %r8, %r8

	r2:
		mrmovq 8(%rdi), %r8
		jle r2_
		iaddq $1, %rax

	r2_:
		rmmovq %r8, 8(%rsi)
		andq %r8, %r8

	r1:
		mrmovq (%rdi), %r8
		jle r1_
		iaddq $1, %rax

	r1_:
		rmmovq %r8, (%rsi)
		andq %r8, %r8
		jle Done
		iaddq $1, %rax

##################################################################
# Do not modify the following section of code
# Function epilogue.
Done:
	ret
##################################################################
# Keep the following label at the end of your function
End:
#/* $end ncopy-ys */
EndFun:

###############################
# Source and destination blocks 
###############################
	.align 8
src:
	.quad 1
	.quad 2
	.quad -3
	.quad -4
	.quad -5
	.quad -6
	.quad 7
	.quad 8
	.quad 9
	.quad 10
	.quad -11
	.quad -12
	.quad -13
	.quad -14
	.quad -15
	.quad -16
	.quad 17
	.quad 18
	.quad -19
	.quad 20
	.quad -21
	.quad -22
	.quad 23
	.quad 24
	.quad 25
	.quad -26
	.quad -27
	.quad -28
	.quad -29
	.quad 30
	.quad 31
	.quad 32
	.quad -33
	.quad -34
	.quad -35
	.quad 36
	.quad -37
	.quad -38
	.quad 39
	.quad -40
	.quad 41
	.quad -42
	.quad -43
	.quad -44
	.quad 45
	.quad 46
	.quad -47
	.quad 48
	.quad 49
	.quad 50
	.quad 51
	.quad -52
	.quad 53
	.quad 54
	.quad -55
	.quad 56
	.quad -57
	.quad -58
	.quad 59
	.quad -60
	.quad 61
	.quad 62
	.quad 63
	.quad 0xbcdefa # This shouldn't get moved

	.align 16
Predest:
	.quad 0xbcdefa
dest:
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
Postdest:
	.quad 0xdefabc

.align 8
# Run time stack
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0

Stack:
